{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Classification:\n\n## Data\n<pre>\n1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \nso from document name, you can extract the label for that document.\n4. Now our problem is to classify all the documents into any one of the class.\n5. Below we provided count plot of all the labels in our data. \n</pre>","metadata":{"id":"mDMgSstPYv0P"}},{"cell_type":"code","source":"### count plot of all the class labels. ","metadata":{"id":"64U9NzWFYv0V","outputId":"f3f19ed2-f637-4a8c-cff7-40a603025e96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assignment:","metadata":{"id":"2mK4TJOFYv0h"}},{"cell_type":"markdown","source":"#### sample document\n<pre>\n<font color='blue'>\nSubject: A word of advice\nFrom: jcopelan@nyx.cs.du.edu (The One and Only)\n\nIn article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n>\n>I've said 100 times that there is no \"alternative\" that should think you\n>might have caught on by now.  And there is no \"alternative\", but the point\n>is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n>and religious knowledge are unsolvable-- or I should say, humans cannot\n>solve them.\n\nHow does that saying go: Those who say it can't be done shouldn't interrupt\nthose who are doing it.\n\nJim\n--\nHave you washed your brain today?\n</font>\n</pre>","metadata":{"id":"VlqYFVI3Yv0k"}},{"cell_type":"markdown","source":"### Preprocessing:\n<pre>\nuseful links: <a href='http://www.pyregex.com/'>http://www.pyregex.com/</a>\n\n<font color='blue'><b>1.</b></font> Find all emails in the document and then get the text after the \"@\". and then split those texts by '.' \nafter that remove the words whose length is less than or equal to 2 and also remove'com' word and then combine those words by space. \nIn one doc, if we have 2 or more mails, get all.\n<b>Eg:[test@dm1.d.com, test2@dm2.dm3.com]-->[dm1.d.com, dm3.dm4.com]-->[dm1,d,com,dm2,dm3,com]-->[dm1,dm2,dm3]-->\"dm1 dm2 dm3\" </b> \nappend all those into one list/array. ( This will give length of 18828 sentences i.e one list for each of the document). \nSome sample output was shown below. \n\n> In the above sample document there are emails [jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu]\n\npreprocessing:\n[jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu] ==> [nyx cs du edu mimsy umd edu cs umd edu] ==> \n[nyx edu mimsy umd edu umd edu]\n\n<font color='blue'><b>2.</b></font> Replace all the emails by space in the original text. \n</pre>","metadata":{"id":"KAR5HoR1Yv0m"}},{"cell_type":"code","source":"# we have collected all emails and preprocessed them, this is sample output\npreprocessed_email","metadata":{"id":"KavKDD9FYv0p","outputId":"0b87ab7b-46df-4995-eaca-4f5831ad223e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(preprocessed_email)","metadata":{"id":"obReqs55Yv0v","outputId":"10770414-9be0-4d63-9587-5363a8c10c4d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<pre>\n<font color='blue'><b>3.</b></font> Get subject of the text i.e. get the total lines where \"Subject:\" occur and remove \nthe word which are before the \":\" remove the newlines, tabs, punctuations, any special chars.\n<b>Eg: if we have sentance like \"Subject: Re: Gospel Dating @ \\r\\r\\n\" --> You have to get \"Gospel Dating\"</b> \nSave all this data into another list/array. \n\n<font color='blue'><b>4.</b></font> After you store it in the list, Replace those sentances in original text by space.\n\n<font color='blue'><b>5.</b></font> Delete all the sentances where sentence starts with <b>\"Write to:\"</b> or <b>\"From:\"</b>.\n> In the above sample document check the 2nd line, we should remove that\n\n<font color='blue'><b>6.</b></font> Delete all the tags like \"< anyword >\"\n> In the above sample document check the 4nd line, we should remove that \"< 65882@mimsy.umd.edu >\"\n\n\n<font color='blue'><b>7.</b></font> Delete all the data which are present in the brackets. \nIn many text data, we observed that, they maintained the explanation of sentence \nor translation of sentence to another language in brackets so remove all those.\n<b>Eg: \"AAIC-The course that gets you HIRED(AAIC - Der Kurs, der Sie anstellt)\" --> \"AAIC-The course that gets you HIRED\"</b>\n\n> In the above sample document check the 4nd line, we should remove that \"(Charley Wingate)\"\n\n\n<font color='blue'><b>8.</b></font> Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".\n\n<font color='blue'><b>9.</b></font> Remove all the words which ends with <b>\":\"</b>.\n<b>Eg: \"Anyword:\"</b>\n> In the above sample document check the 4nd line, we should remove that \"writes:\"\n\n\n<font color='blue'><b>10.</b></font> Decontractions, replace words like below to full words. \nplease check the donors choose preprocessing for this \n<b>Eg: can't -> can not, 's -> is, i've -> i have, i'm -> i am, you're -> you are, i'll --> i will </b>\n\n<b> There is no order to do point 6 to 10. but you have to get final output correctly</b>\n\n<font color='blue'><b>11.</b></font> Do chunking on the text you have after above preprocessing. \nText chunking, also referred to as shallow parsing, is a task that \nfollows Part-Of-Speech Tagging and that adds more structure to the sentence.\nSo it combines the some phrases, named entities into single word.\nSo after that combine all those phrases/named entities by separating <b>\"_\"</b>. \nAnd remove the phrases/named entities if that is a \"Person\". \nYou can use <b>nltk.ne_chunk</b> to get these. \nBelow we have given one example. please go through it. \n\nuseful links: \n<a href='https://www.nltk.org/book/ch07.html'>https://www.nltk.org/book/ch07.html</a>\n<a href='https://stackoverflow.com/a/31837224/4084039'>https://stackoverflow.com/a/31837224/4084039</a>\n<a href='http://www.nltk.org/howto/tree.html'>http://www.nltk.org/howto/tree.html</a>\n<a href='https://stackoverflow.com/a/44294377/4084039'>https://stackoverflow.com/a/44294377/4084039</a>\n</pre>","metadata":{"id":"zIovFDQzYv03"}},{"cell_type":"code","source":"#i am living in the New York\nprint(\"i am living in the New York -->\", list(chunks))\nprint(\" \")\nprint(\"-\"*50)\nprint(\" \")\n#My name is Srikanth Varma\nprint(\"My name is Srikanth Varma -->\", list(chunks1))","metadata":{"id":"2lAaKQ6EYv04","outputId":"53b66a94-acef-4002-e51c-002bde4178b4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<pre>We did chunking for above two lines and then We got one list where each word is mapped to a \nPOS(parts of speech) and also if you see \"New York\" and \"Srikanth Varma\", \nthey got combined and represented as a tree and \"New York\" was referred as \"GPE\" and \"Srikanth Varma\" was referred as \"PERSON\". \nso now you have to Combine the \"New York\" with <b>\"_\"</b> i.e \"New_York\"\nand remove the \"Srikanth Varma\" from the above sentence because it is a person.</pre>","metadata":{"id":"XV8gzLUjYv0-"}},{"cell_type":"markdown","source":"<pre>\n<font color='blue'><b>13.</b></font> Replace all the digits with space i.e delete all the digits. \n> In the above sample document, the 6th line have digit 100, so we have to remove that.\n\n<font color='blue'><b>14.</b></font> After doing above points, we observed there might be few word's like\n <b> \"_word_\" (i.e starting and ending with the _), \"_word\" (i.e starting with the _),\n  \"word_\" (i.e ending with the _)</b> remove the <b>_</b> from these type of words. \n\n<font color='blue'><b>15.</b></font>  We also observed some words like <b> \"OneLetter_word\"- eg: d_berlin, \n\"TwoLetters_word\" - eg: dr_berlin </b>, in these words we remove the \"OneLetter_\" (d_berlin ==> berlin) and \n\"TwoLetters_\" (de_berlin ==> berlin). i.e remove the words \nwhich are length less than or equal to 2 after spliiting those words by \"_\". \n\n<font color='blue'><b>16.</b></font> Convert all the words into lower case and lowe case \nand remove the words which are greater than or equal to 15 or less than or equal to 2.\n\n<font color='blue'><b>17.</b></font> replace all the words except \"A-Za-z_\" with space. \n\n<font color='blue'><b>18.</b></font> Now You got Preprocessed Text, email, subject. create a dataframe with those. \nBelow are the columns of the df. \n</pre>","metadata":{"id":"VpaC-KF3Yv1A"}},{"cell_type":"code","source":"data = pd.read_csv('')","metadata":{"id":"SCRqvEyizFkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"id":"hB43OGEfYv1C","outputId":"945bc8a4-1f99-4410-94c8-c776a405b5f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[400]","metadata":{"id":"AM6A19xFYv1I","outputId":"9de13fa8-6604-49a2-8013-6b22f0a256a8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### To get above mentioned data frame --> Try to Write Total Preprocessing steps in One Function Named Preprocess as below. ","metadata":{"id":"rfWUeIN1Yv1N"}},{"cell_type":"code","source":"def preprocess(Input_Text):\n    \"\"\"Do all the Preprocessing as shown above and\n    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data\"\"\"\n    return (list_of_preproessed_emails,subject,text)","metadata":{"id":"uEGEHTNQYv1N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code checking:\n\n<font color='red' size=4>\nAfter Writing preprocess function. call that functoin with the input text of 'alt.atheism_49960' doc and print the output of the preprocess function\n<br>\nThis will help us to evaluate faster, based on the output we can suggest you if there are any changes.\n</font>","metadata":{"id":"ceASjKizYv1U"}},{"cell_type":"markdown","source":"### After writing Preprocess function, call the function for each of the document(18828 docs) and then create a dataframe as mentioned above.","metadata":{"id":"2x3og_iaYv1S"}},{"cell_type":"markdown","source":"### Training The models to Classify: \n\n<pre>\n1. Combine \"preprocessed_text\", \"preprocessed_subject\", \"preprocessed_emails\" into one column. use that column to model. \n\n2. Now Split the data into Train and test. use 25% for test also do a stratify split. \n\n3. Analyze your text data and pad the sequnce if required. \nSequnce length is not restricted, you can use anything of your choice. \nyou need to give the reasoning\n\n4. Do Tokenizer i.e convert text into numbers. please be careful while doing it. \nif you are using tf.keras \"Tokenizer\" API, it removes the <b>\"_\"</b>, but we need that.\n\n5. code the model's ( Model-1, Model-2 ) as discussed below \nand try to optimize that models.  \n\n6. For every model use predefined Glove vectors. \n<b>Don't train any word vectors while Training the model.</b>\n\n7. Use \"categorical_crossentropy\" as Loss. \n\n8. Use <b>Accuracy and Micro Avgeraged F1 score</b> as your as Key metrics to evaluate your model. \n\n9.  Use Tensorboard to plot the loss and Metrics based on the epoches.\n\n10. Please save your best model weights in to <b>'best_model_L.h5' ( L = 1 or 2 )</b>. \n\n11. You are free to choose any Activation function, learning rate, optimizer.\nBut have to use the same architecture which we are giving below.\n\n12. You can add some layer to our architecture but you <b>deletion</b> of layer is not acceptable.\n\n13. Try to use <b>Early Stopping</b> technique or any of the callback techniques that you did in the previous assignments.\n\n14. For Every model save your model to image ( Plot the model) with shapes \nand inlcude those images in the notebook markdown cell, \nupload those imgages to Classroom. You can use \"plot_model\" \nplease refer <a href='https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model'>this</a> if you don't know how to plot the model with shapes. \n\n</pre>","metadata":{"id":"n3ucJLtWYv1V"}},{"cell_type":"code","source":"import os\ndocuments=os.listdir('../input/docudocu/documents')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:28:13.739512Z","iopub.execute_input":"2022-04-03T09:28:13.740031Z","iopub.status.idle":"2022-04-03T09:28:14.118037Z","shell.execute_reply.started":"2022-04-03T09:28:13.739929Z","shell.execute_reply":"2022-04-03T09:28:14.117285Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#getting data\ndata_all=[]\ndata=[]\nfor i in documents:\n    with open('../input/docudocu/documents/'+str(i),'r',encoding='utf8',errors='replace')as file:\n        fc=file.read()\n        data.append(fc)\ndata_all.append(data)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:28:16.960901Z","iopub.execute_input":"2022-04-03T09:28:16.961609Z","iopub.status.idle":"2022-04-03T09:29:03.596301Z","shell.execute_reply.started":"2022-04-03T09:28:16.961550Z","shell.execute_reply":"2022-04-03T09:29:03.595439Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Dense,MaxPooling1D\nfrom tensorflow.keras.layers import Flatten,Dropout,concatenate,Input\nfrom tensorflow.keras.layers import Embedding\nimport datetime\nimport numpy as np\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.callbacks import TensorBoard\nimport tensorflow as tf\nimport itertools\nimport re\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"C6vz5A1wUFuD","execution":{"iopub.status.busy":"2022-04-03T09:29:03.598167Z","iopub.execute_input":"2022-04-03T09:29:03.598637Z","iopub.status.idle":"2022-04-03T09:29:10.632117Z","shell.execute_reply.started":"2022-04-03T09:29:03.598591Z","shell.execute_reply":"2022-04-03T09:29:10.631159Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\nnltk.download('averaged_perceptron_tagger')","metadata":{"id":"jFgGZCNTg4Y6","execution":{"iopub.status.busy":"2022-04-03T09:29:10.633735Z","iopub.execute_input":"2022-04-03T09:29:10.634078Z","iopub.status.idle":"2022-04-03T09:29:11.457751Z","shell.execute_reply.started":"2022-04-03T09:29:10.634036Z","shell.execute_reply":"2022-04-03T09:29:11.456947Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"eVNq-_XdU_V6","outputId":"d22d15c3-1858-4988-9282-0a31aae092e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unrar x \"/content/drive/MyDrive/documents.rar\" \"/content/drive/MyDrive/\"","metadata":{"id":"wIIkQyG60uvA","outputId":"5838c2c9-f272-49ed-f0d6-78451da8a730"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fetching the data\ncomp_data = []\ndoc = []\ndocuments=os.listdir('/content/drive/MyDrive/documents')\nfor d in documents:\n    with open('/content/drive/MyDrive/documents/' + str(d),'r',encoding='utf8',errors='replace') as f:\n        file_read = f.read()\n        doc.append(file_read)\n        \ncomp_data.append(doc)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting data\ncomp_data=[]\ndoc=[]\nfor i in documents:\n    with open('../input/docudocu/documents/'+str(i),'r',encoding='utf8',errors='replace')as file:\n        fc=file.read()\n        doc.append(fc)\ncomp_data.append(doc)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:29:28.241245Z","iopub.execute_input":"2022-04-03T09:29:28.241660Z","iopub.status.idle":"2022-04-03T09:29:36.646993Z","shell.execute_reply.started":"2022-04-03T09:29:28.241620Z","shell.execute_reply":"2022-04-03T09:29:36.646244Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"emails=[]\nmail=[]\nfor d in comp_data[0]:\n    email = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', d)\n    for i in email:\n        d=d.replace(i,\" \")\n    mail.append(d)\n    emails.append(email)\nl1=[]\nfor i in emails:\n    l2=[]\n    for email in i:\n        for j in range(len(email)):\n            if(email[j]=='@'):\n                text=email[j+1:].split('.')\n                l2.append(text)\n    l1.append(l2)\nl3=[]\nfor i in l1:\n    l2=[]\n    for j in i:\n        for word in j:\n            if((len(word)>2) & (word!='com')):\n                l2.append(word)\n    l3.append(l2)\npreprocessed_email=[]\nfor i in l3:\n    s=\"\"\n    s=\" \".join(i)\n    preprocessed_email.append(s)\n#text=email[email.find(\"@\")+1:].split('.')","metadata":{"id":"qFuXb7Ny1Sq8","execution":{"iopub.status.busy":"2022-04-03T09:29:36.674214Z","iopub.execute_input":"2022-04-03T09:29:36.675060Z","iopub.status.idle":"2022-04-03T09:29:41.518491Z","shell.execute_reply.started":"2022-04-03T09:29:36.675009Z","shell.execute_reply":"2022-04-03T09:29:41.517757Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Removing subject\nsubject=[]\nwo_sub_data=[]\nfor i in mail:\n    c = re.findall('Subject:.*',i)\n    sub= c\n    for j in sub:\n        str_j = str(j)\n        i=i.replace(str_j,\" \")\n    wo_sub_data.append(i)\n    subject.append(sub)","metadata":{"id":"6qHYSie70wm7","execution":{"iopub.status.busy":"2022-04-03T09:29:41.520666Z","iopub.execute_input":"2022-04-03T09:29:41.521188Z","iopub.status.idle":"2022-04-03T09:29:41.680437Z","shell.execute_reply.started":"2022-04-03T09:29:41.521140Z","shell.execute_reply":"2022-04-03T09:29:41.679780Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"subject_pp=[]\n#Removing pucntuation https://stackoverflow.com/questions/5843518/remove-all-special-characters-punctuation-and-spaces-from-string\n#print(len(subject))\nfor s in subject:\n    for word in s:\n        word=word.replace(\"Subject: \",'')\n        word=re.sub(r'[^a-zA-Z0-9 ]',r'',word)\n        word=word.replace(\"Re: \",'')\n    subject_pp.append(word)","metadata":{"id":"F5HuyFJKXlQb","execution":{"iopub.status.busy":"2022-04-03T09:29:41.681760Z","iopub.execute_input":"2022-04-03T09:29:41.682258Z","iopub.status.idle":"2022-04-03T09:29:41.764796Z","shell.execute_reply.started":"2022-04-03T09:29:41.682214Z","shell.execute_reply":"2022-04-03T09:29:41.764188Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_wo_write_from=[]\nfor txt in wo_sub_data:\n    f=re.findall(\"Write to:.*\",txt)\n    g=re.findall(\"From:.*\",txt)\n    for i in f:\n        txt=txt.replace(str(i),'')\n    for j in g:\n        txt=txt.replace(str(j),'')\n    data_wo_write_from.append(txt)\n","metadata":{"id":"5O-c8c3uXlSJ","execution":{"iopub.status.busy":"2022-04-03T09:29:41.765891Z","iopub.execute_input":"2022-04-03T09:29:41.766216Z","iopub.status.idle":"2022-04-03T09:29:41.924319Z","shell.execute_reply.started":"2022-04-03T09:29:41.766188Z","shell.execute_reply":"2022-04-03T09:29:41.923381Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_wo_colon=[]\nfor i in data_wo_write_from:\n    i_lst = i.split()\n    for j in i_lst:\n        if(j[-1]==':'):\n            str_j = str(j)\n            i=i.replace(str_j,'')\n    data_wo_colon.append(i)\n","metadata":{"id":"NGS_mt38XlWN","execution":{"iopub.status.busy":"2022-04-03T09:29:41.925633Z","iopub.execute_input":"2022-04-03T09:29:41.925881Z","iopub.status.idle":"2022-04-03T09:29:43.231319Z","shell.execute_reply.started":"2022-04-03T09:29:41.925848Z","shell.execute_reply":"2022-04-03T09:29:43.230555Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#https://stackoverflow.com/questions/54396405/how-can-i-preprocess-nlp-text-lowercase-remove-special-characters-remove-numb\ndata_wo_contr=[]\nr = 8\nfor phrase in data_wo_colon:\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    data_wo_contr.append(phrase)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:29:43.232264Z","iopub.execute_input":"2022-04-03T09:29:43.233043Z","iopub.status.idle":"2022-04-03T09:29:43.704452Z","shell.execute_reply.started":"2022-04-03T09:29:43.233011Z","shell.execute_reply":"2022-04-03T09:29:43.703625Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data_wo_tag=[]\nfor i in data_wo_contr:\n    s=re.findall('<.*?>',i)\n    for j in s:\n        i=i.replace(str(j),'')\n    s=re.findall('\\(.*\\)',i)\n    for j in s:\n        i=i.replace(str(j),'')\n    s=re.findall('\\n',i)\n    for j in s:\n        i=i.replace(str(j),'')\n    s=re.findall('\\t',i)\n    for j in s:\n        i=i.replace(str(j),'')\n    i = re.sub(r'[>|?|\\|.|-]',r'',i)\n    i = re.sub(r'[=|!|<]',r'',i)\n    data_wo_tag.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:29:54.921679Z","iopub.execute_input":"2022-04-03T09:29:54.921942Z","iopub.status.idle":"2022-04-03T09:30:06.145569Z","shell.execute_reply.started":"2022-04-03T09:29:54.921898Z","shell.execute_reply":"2022-04-03T09:30:06.144651Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data_wo_tag[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:30:06.146955Z","iopub.execute_input":"2022-04-03T09:30:06.147304Z","iopub.status.idle":"2022-04-03T09:30:06.154678Z","shell.execute_reply.started":"2022-04-03T09:30:06.147261Z","shell.execute_reply":"2022-04-03T09:30:06.153981Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"' In article ,      Alas, we too will have to stop using it  BrianWe where following version of xv and I have been very surprise to discover thatthe new version is a  What a pitty  :( What I found on the Inthernet was the freeware I make myself a freeware andI spent long time on it but I do not plain to make paid to use it I thinkif evrybody spent some time to make freeware, evrybody will be paid by the useof other freewareHere we will stay with XV 2 and drop XV 3 S Meunier  '"},"metadata":{}}]},{"cell_type":"code","source":"#file:///C:/Users/LENOVO/Desktop/ch/Text_Classification_Assignment_colab.pdf.pdf\nfrom nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk import Tree\ndef get_contgp_chunks(text, label):\n    \n    prev = None\n    continuous_chunk = []\n    word=[]\n    continuos_word=[]\n    current_chunk = []\n    i=0\n    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n    for subtree in chunked:\n        if type(subtree) == Tree and subtree.label() == label:\n            i+=1\n            current_chunk.append(\"_\".join([token for token, pos in subtree.leaves()]))\n            word.append(\" \".join([token for token, pos in subtree.leaves()]))\n        if current_chunk:\n            named_entity = \" \".join(current_chunk)\n            named_word=\" \".join(word)\n            named_word=\" \".join(word)\n            flag = False\n            if named_entity not in continuous_chunk:\n                continuous_chunk.append(named_entity)\n                current_chunk = []\n            if named_word not in continuos_word:\n                continuos_word.append(named_word)\n                word=[]\n        else:\n            continue\n    return continuous_chunk,continuos_word\n\ndef get_contin_chunks(text, label):\n    flag = True\n    chunks_cont = []\n    current_chunk = []\n    prev = None\n    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n    for subtree in chunked:\n        \n        if type(subtree) == Tree and subtree.label() == label:\n            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n        if current_chunk:\n            named_entity = \" \".join(current_chunk)\n            if named_entity not in chunks_cont:\n                chunks_cont.append(named_entity)\n                current_chunk = []\n        else:\n            continue\n    return chunks_cont\n","metadata":{"id":"p3dAuoHaXlna","execution":{"iopub.status.busy":"2022-04-03T08:40:19.430641Z","iopub.execute_input":"2022-04-03T08:40:19.430908Z","iopub.status.idle":"2022-04-03T08:40:19.445919Z","shell.execute_reply.started":"2022-04-03T08:40:19.430886Z","shell.execute_reply":"2022-04-03T08:40:19.444797Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"chunked_data=[]\nfor i in data_wo_contr:\n    gt=get_contin_chunks(i, \"PERSON\")\n    for j in gt:\n        i=i.replace(str(j),'')\n    g,w=get_contgp_chunks(i, \"GPE\")\n    for k in range(len(g)):\n        rep_val =i.replace(w[k],g[k])\n        i= rep_val\n    chunked_data.append(i)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunked_data[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk import Tree\ndef contin_chunks(text, label):\n    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n    prev = None\n    continuous_chunk = []\n    current_chunk = []\n    for subtree in chunked:\n        if type(subtree) == Tree and subtree.label() == label:\n            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n        if current_chunk:\n            named_entity = \" \".join(current_chunk)\n            if named_entity not in continuous_chunk:\n                continuous_chunk.append(named_entity)\n                current_chunk = []\n        else:\n            continue\n    return continuous_chunk\n\ndef contingp_chunks(text, label):\n    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n    prev = None\n    continuous_chunk = []\n    word=[]\n    continuos_word=[]\n    current_chunk = []\n    i=0\n    for subtree in chunked:\n        if type(subtree) == Tree and subtree.label() == label:\n            i+=1\n            current_chunk.append(\"_\".join([token for token, pos in subtree.leaves()]))\n            word.append(\" \".join([token for token, pos in subtree.leaves()]))\n        if current_chunk:\n            named_entity = \" \".join(current_chunk)\n            named_word=\" \".join(word)\n            named_word=\" \".join(word)\n            if named_entity not in continuous_chunk:\n                continuous_chunk.append(named_entity)\n                current_chunk = []\n            if named_word not in continuos_word:\n                continuos_word.append(named_word)\n                word=[]\n        else:\n            continue\n    return continuous_chunk,continuos_word","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:30:33.965447Z","iopub.execute_input":"2022-04-03T09:30:33.965715Z","iopub.status.idle":"2022-04-03T09:30:33.980639Z","shell.execute_reply.started":"2022-04-03T09:30:33.965685Z","shell.execute_reply":"2022-04-03T09:30:33.980044Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:32:05.410571Z","iopub.execute_input":"2022-04-03T09:32:05.410840Z","iopub.status.idle":"2022-04-03T09:32:05.414980Z","shell.execute_reply.started":"2022-04-03T09:32:05.410802Z","shell.execute_reply":"2022-04-03T09:32:05.414029Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"chunked_data=[]\nfor i in tqdm(data_wo_tag):\n    p=contin_chunks(i, \"PERSON\")\n    for j in p:\n        i=i.replace(str(j),'')\n    g,w=contingp_chunks(i, \"GPE\")\n    for k in range(len(g)):\n        i=i.replace(w[k],g[k])\n    chunked_data.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T09:32:08.599773Z","iopub.execute_input":"2022-04-03T09:32:08.600061Z","iopub.status.idle":"2022-04-03T10:36:38.782061Z","shell.execute_reply.started":"2022-04-03T09:32:08.600030Z","shell.execute_reply":"2022-04-03T10:36:38.780651Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 18828/18828 [1:04:30<00:00,  4.86it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"data_wo_unscore=[]\nfor i in chunked_data:\n    txt_lst = i.split()\n    for word in txt_lst:\n        if word.startswith('_'):\n            i=i.replace(word[0],\"\")\n        if word.endswith(\"_\"):\n            i=i.replace(word[-1],\"\")\n    data_wo_unscore.append(i)\n","metadata":{"id":"CW4Hpe-zg_Un","execution":{"iopub.status.busy":"2022-04-03T10:39:57.921147Z","iopub.execute_input":"2022-04-03T10:39:57.921681Z","iopub.status.idle":"2022-04-03T10:39:59.613715Z","shell.execute_reply.started":"2022-04-03T10:39:57.921636Z","shell.execute_reply":"2022-04-03T10:39:59.612842Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"data_wo_dig=[]\nfor i in data_wo_unscore:\n    digits=re.findall('[0-9]+',i)\n    for d in digits:\n        rep_val =i.replace(d,\"\")\n        i=rep_val\n    data_wo_dig.append(i)\n","metadata":{"id":"7oCk6-Wsg_S_","execution":{"iopub.status.busy":"2022-04-03T10:43:23.837302Z","iopub.execute_input":"2022-04-03T10:43:23.837612Z","iopub.status.idle":"2022-04-03T10:43:29.463560Z","shell.execute_reply.started":"2022-04-03T10:43:23.837582Z","shell.execute_reply":"2022-04-03T10:43:29.462773Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#LowerCase\ndata_lc=[]\nfor i in data_wo_dig:\n    txt_lst = i.split()\n    j= ' '.join([w for w in txt_lst if len(w)>2])\n    j_lst  = j.split()\n    i_new=' '.join([w for w in j_lst if (len(w)<=15)])\n    data_lc.append(i_new.lower())","metadata":{"id":"5wLtofi5g_Zn","execution":{"iopub.status.busy":"2022-04-03T10:45:56.476755Z","iopub.execute_input":"2022-04-03T10:45:56.477153Z","iopub.status.idle":"2022-04-03T10:45:58.183341Z","shell.execute_reply.started":"2022-04-03T10:45:56.477113Z","shell.execute_reply":"2022-04-03T10:45:58.182483Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"preprocessed_text=[]\nfor txt in data_lc:\n    val_to_put = re.sub(r'[^A-Za-z_ ]','',i)\n    txt=val_to_put\n    preprocessed_text.append(txt)\n","metadata":{"id":"0gCE3yQhg_bM","execution":{"iopub.status.busy":"2022-04-03T10:47:52.647004Z","iopub.execute_input":"2022-04-03T10:47:52.648203Z","iopub.status.idle":"2022-04-03T10:47:55.500752Z","shell.execute_reply.started":"2022-04-03T10:47:52.648155Z","shell.execute_reply":"2022-04-03T10:47:55.500122Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"preprocessed_text[0]","metadata":{"id":"DW4jEUflg_fD","execution":{"iopub.status.busy":"2022-04-03T10:48:11.305980Z","iopub.execute_input":"2022-04-03T10:48:11.306485Z","iopub.status.idle":"2022-04-03T10:48:11.311474Z","shell.execute_reply.started":"2022-04-03T10:48:11.306435Z","shell.execute_reply":"2022-04-03T10:48:11.310841Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'  I have sent this to Mr  privately Post it onlyif you think it of general interestHere is a copy of something I wrote for another list You mayfind it relevantA listmember   What makes commonlaw marriages wrongA commonlaw marriage is not necessarily wrong in itself There isnothing in the Bible  about getting married bya preacher or by a priest  And in fact Jewishpriests have never had any connection with weddingsThere is a common notion that the marriage is performed by theclergyman In fact the traditional Christian view at least in theWest is that the bride and groom are the ministers of the marriageand that the clergyman is there only as a witnessHOWEVERThe essential ingredient of a marriage is mutual commitment Twopersons are considered to be married if and only if they have boundthemselves by mutual promises to live together as husband and wifeforsaking all others till death do them part      The reason why those who have reason to be concerned about whois married to whom have always insisted on some kind of publicceremony is in order that society and the couple themselves may beclear about whether a commitment has been madeSuppose that we do away with the public ceremony the standard vowsetc Instead we have a man and a woman settling down to livetogether      After a year or so the man says to the  Hey honey itwas great while it lasted but I think it is time to move on      She  What are you talking about      He  I am leaving you and looking for someone prettier andyounger      She  But you can not We are married      He  What are you talking about We never got married      She  I remember distinctly what you said to me the nightwe first made love You  My love for you is as deep as theocean as eternal as the stars As long as I live I am yoursutterly and completely When I lie on my deathbed my last feeblebreath will utter your name My      He  Oh that That was just rhetoric Just poetry When aman is in a romantic mood he is bound to say all kinds of sillythings like that You must not take them literallyAnd that is why you have an insistence on a formal ceremony that isa matter of public record      The Church insists on it because it is her duty among otherthings to give moral advice and you cannot give a man moral adviceabout his relations with a woman if you have no idea who is marriedto whom if anybody and vice versa      The State insists on it since the state has a concern withproperty rights with child care and support and therefore needs toknow who has made what commitments to whom      Prospective fathersinlaw insist on it because they do notwant their daughters seduced and abandoned      Prospective spouses insist on it because they want to makesure they know whether what they are hearing is a real commitmentor just poetry      And persons making vows themselves insist on making themformally and publicly in order that they may be clear in their ownminds about what it is that they are doing and may know themselvesthat this is not just rhetoric This is the real thing      Hence the insistence on a formal public explicit avowal of themarriage commitment  The Church goes further and insists that whenChristians marry a clergyman shall be present at the wedding andrecord the vows on behalf of the Church not because it isimpossible to have a valid wedding without a clergyman but in orderto make sure that the couple understand what the Christian teachingabout marriage is and that they are in fact promising to be marriedin a Christian sense The Church also prefers a standard marriagevow and is wary of letting couples Write their own vows for muchthe same reason that lawyers prefer standard terminology when theydraw up a will or a contract Certain language has been repeatedlyused in wills and one can be sure how the courts will interpret itTry to say the same thing in your own words and you may find thatthe probate judge is interpretation of them is not at all what youintended  Similarly the Church prefers to avoid endless debatesabout whether You are my main squeeze and I am here for the longhaul do in fact cover the same territory as forsaking all othersand till death do us part      This topic has come up on the list before Is there any topicthat has not One listmember was asking If a couple love eachother and are living together is not that marriage in the eyes of Eventually someone asked In that case what is their statusif they break up Is that the moral equivalent of getting a divorceAre they in a relationship that  forbids either of them to walkout on  The original questioner  Good grief I never thoughtof that In fact there are reasonable grounds for suspecting thatsomeone who says We do not need a piece of paper or a ceremony infront of a judge or a preacher in order to show that we love eachother is trying to have it both ways  to have the advantages ofmarriage plus the option of changing his mind with a minimum ofbotherAt this point someone may say None of this applies to me and mymate We are quite clear on the fact that we have assumed a lifelongcommitment for better or worse forsaking all others till deathus do part So in our case no ceremony is needed     To this my reply would  The reason for requiring a driver islicense is to keep dangerous drivers off the road  What is wrong initself is not the existence of unlicensed drivers but the existenceof dangerous drivers However testing and licensing drivers is anobvious and reasonable means of pursuing the goal of reducing thenumber of dangerous drivers on the road Therefore the State rightlymakes and enforces such laws and you the citizen have a positivemoral obligation to refrain from driving without a license no matterhow much of a hotshot behind the wheel you think you areBack to the original question We have a listmember who knows acouple who have been living together for around  years He At what point did they stop fornicating and start being married I at the point if any where they both definitely andexplicitly accepted an obligation to be faithful to each other forbetter or worse as long as they both lived If they have acceptedsuch an obligation what are their reasons for not being willing todeclare it in front of say a justice of the peace Yours '"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Preprocessed Email\")\nprint(preprocessed_email[documents.index(\"alt.atheism_49960.txt\")])\nprint(\"-\"*100)\nprint(\"Preprocessed Subject\")\nprint(subject_pp[documents.index(\"alt.atheism_49960.txt\")])\nprint(\"-\"*100)\nprint(\"Preprocessed Text\")\nprint(preprocessed_text[documents.index(\"alt.atheism_49960.txt\")])","metadata":{"id":"98uRupqu0wwL","execution":{"iopub.status.busy":"2022-04-03T10:51:46.036435Z","iopub.execute_input":"2022-04-03T10:51:46.036828Z","iopub.status.idle":"2022-04-03T10:51:46.048402Z","shell.execute_reply.started":"2022-04-03T10:51:46.036788Z","shell.execute_reply":"2022-04-03T10:51:46.046793Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Preprocessed Email\nmantis netcom mantis\n----------------------------------------------------------------------------------------------------\nPreprocessed Subject\nAltAtheism FAQ Atheist Resources\n----------------------------------------------------------------------------------------------------\nPreprocessed Text\n  I have sent this to Mr  privately Post it onlyif you think it of general interestHere is a copy of something I wrote for another list You mayfind it relevantA listmember   What makes commonlaw marriages wrongA commonlaw marriage is not necessarily wrong in itself There isnothing in the Bible  about getting married bya preacher or by a priest  And in fact Jewishpriests have never had any connection with weddingsThere is a common notion that the marriage is performed by theclergyman In fact the traditional Christian view at least in theWest is that the bride and groom are the ministers of the marriageand that the clergyman is there only as a witnessHOWEVERThe essential ingredient of a marriage is mutual commitment Twopersons are considered to be married if and only if they have boundthemselves by mutual promises to live together as husband and wifeforsaking all others till death do them part      The reason why those who have reason to be concerned about whois married to whom have always insisted on some kind of publicceremony is in order that society and the couple themselves may beclear about whether a commitment has been madeSuppose that we do away with the public ceremony the standard vowsetc Instead we have a man and a woman settling down to livetogether      After a year or so the man says to the  Hey honey itwas great while it lasted but I think it is time to move on      She  What are you talking about      He  I am leaving you and looking for someone prettier andyounger      She  But you can not We are married      He  What are you talking about We never got married      She  I remember distinctly what you said to me the nightwe first made love You  My love for you is as deep as theocean as eternal as the stars As long as I live I am yoursutterly and completely When I lie on my deathbed my last feeblebreath will utter your name My      He  Oh that That was just rhetoric Just poetry When aman is in a romantic mood he is bound to say all kinds of sillythings like that You must not take them literallyAnd that is why you have an insistence on a formal ceremony that isa matter of public record      The Church insists on it because it is her duty among otherthings to give moral advice and you cannot give a man moral adviceabout his relations with a woman if you have no idea who is marriedto whom if anybody and vice versa      The State insists on it since the state has a concern withproperty rights with child care and support and therefore needs toknow who has made what commitments to whom      Prospective fathersinlaw insist on it because they do notwant their daughters seduced and abandoned      Prospective spouses insist on it because they want to makesure they know whether what they are hearing is a real commitmentor just poetry      And persons making vows themselves insist on making themformally and publicly in order that they may be clear in their ownminds about what it is that they are doing and may know themselvesthat this is not just rhetoric This is the real thing      Hence the insistence on a formal public explicit avowal of themarriage commitment  The Church goes further and insists that whenChristians marry a clergyman shall be present at the wedding andrecord the vows on behalf of the Church not because it isimpossible to have a valid wedding without a clergyman but in orderto make sure that the couple understand what the Christian teachingabout marriage is and that they are in fact promising to be marriedin a Christian sense The Church also prefers a standard marriagevow and is wary of letting couples Write their own vows for muchthe same reason that lawyers prefer standard terminology when theydraw up a will or a contract Certain language has been repeatedlyused in wills and one can be sure how the courts will interpret itTry to say the same thing in your own words and you may find thatthe probate judge is interpretation of them is not at all what youintended  Similarly the Church prefers to avoid endless debatesabout whether You are my main squeeze and I am here for the longhaul do in fact cover the same territory as forsaking all othersand till death do us part      This topic has come up on the list before Is there any topicthat has not One listmember was asking If a couple love eachother and are living together is not that marriage in the eyes of Eventually someone asked In that case what is their statusif they break up Is that the moral equivalent of getting a divorceAre they in a relationship that  forbids either of them to walkout on  The original questioner  Good grief I never thoughtof that In fact there are reasonable grounds for suspecting thatsomeone who says We do not need a piece of paper or a ceremony infront of a judge or a preacher in order to show that we love eachother is trying to have it both ways  to have the advantages ofmarriage plus the option of changing his mind with a minimum ofbotherAt this point someone may say None of this applies to me and mymate We are quite clear on the fact that we have assumed a lifelongcommitment for better or worse forsaking all others till deathus do part So in our case no ceremony is needed     To this my reply would  The reason for requiring a driver islicense is to keep dangerous drivers off the road  What is wrong initself is not the existence of unlicensed drivers but the existenceof dangerous drivers However testing and licensing drivers is anobvious and reasonable means of pursuing the goal of reducing thenumber of dangerous drivers on the road Therefore the State rightlymakes and enforces such laws and you the citizen have a positivemoral obligation to refrain from driving without a license no matterhow much of a hotshot behind the wheel you think you areBack to the original question We have a listmember who knows acouple who have been living together for around  years He At what point did they stop fornicating and start being married I at the point if any where they both definitely andexplicitly accepted an obligation to be faithful to each other forbetter or worse as long as they both lived If they have acceptedsuch an obligation what are their reasons for not being willing todeclare it in front of say a justice of the peace Yours \n","output_type":"stream"}]},{"cell_type":"code","source":"#Fetching the labels\ntxt_labels = []\nfor d in documents:\n    each_label = d.split('_')\n    val = each_label[0]\n    txt_labels.append(val)\n    \nlabels_txt = set(txt_labels)\nprint(labels_txt)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T10:57:26.740494Z","iopub.execute_input":"2022-04-03T10:57:26.740766Z","iopub.status.idle":"2022-04-03T10:57:26.762283Z","shell.execute_reply.started":"2022-04-03T10:57:26.740738Z","shell.execute_reply":"2022-04-03T10:57:26.761659Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"{'comp.graphics', 'talk.politics.guns', 'alt.atheism', 'sci.med', 'comp.windows.x', 'rec.motorcycles', 'talk.religion.misc', 'soc.religion.christian', 'rec.sport.hockey', 'sci.electronics', 'sci.crypt', 'talk.politics.misc', 'sci.space', 'talk.politics.mideast', 'rec.sport.baseball', 'comp.os.ms-windows.misc', 'misc.forsale', 'rec.autos', 'comp.sys.mac.hardware', 'comp.sys.ibm.pc.hardware'}\n","output_type":"stream"}]},{"cell_type":"code","source":"len(labels_txt)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T10:58:36.495827Z","iopub.execute_input":"2022-04-03T10:58:36.496519Z","iopub.status.idle":"2022-04-03T10:58:36.502730Z","shell.execute_reply.started":"2022-04-03T10:58:36.496476Z","shell.execute_reply":"2022-04-03T10:58:36.501802Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}]},{"cell_type":"code","source":"doc_txt = comp_data[0]\npreprocessed_data=pd.DataFrame({\"text\":doc_txt,\"class\":txt_labels,\"preprocessed_text\":preprocessed_text,\n                                \"preprocessed_subject\":subject_pp,\"preprocessed_emails\":preprocessed_email})\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T11:02:57.770707Z","iopub.execute_input":"2022-04-03T11:02:57.771663Z","iopub.status.idle":"2022-04-03T11:02:57.799614Z","shell.execute_reply.started":"2022-04-03T11:02:57.771605Z","shell.execute_reply":"2022-04-03T11:02:57.799006Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"preprocessed_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T11:02:58.094424Z","iopub.execute_input":"2022-04-03T11:02:58.094745Z","iopub.status.idle":"2022-04-03T11:02:58.108338Z","shell.execute_reply.started":"2022-04-03T11:02:58.094711Z","shell.execute_reply":"2022-04-03T11:02:58.107665Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"                                                text                    class  \\\n0  From: rats@cbnewsc.cb.att.com (Morris the Cat)...       talk.politics.guns   \n1  From: meunier@inf.enst.fr (Sylvain Meunier)\\nS...           comp.windows.x   \n2  From: bitzm@columbia.dsu.edu (MICHAEL BITZ)\\nS...             misc.forsale   \n3  Subject: Re: Protective gear\\nFrom: bclarke@ga...          rec.motorcycles   \n4  From: jls@antares. (Jon Sweet)\\nSubject: Re: V...  comp.os.ms-windows.misc   \n\n                                   preprocessed_text  \\\n0    I have sent this to Mr  privately Post it on...   \n1    I have sent this to Mr  privately Post it on...   \n2    I have sent this to Mr  privately Post it on...   \n3    I have sent this to Mr  privately Post it on...   \n4    I have sent this to Mr  privately Post it on...   \n\n               preprocessed_subject  \\\n0  VancouverSeattle Study Critiques   \n1             Re XV 300 has escaped   \n2             1 meg SIMMS for sale    \n3                Re Protective gear   \n4                    Re VBRUN100dll   \n\n                                 preprocessed_emails  \n0                                        cbnewsc att  \n1  inf enst colobus nrao edu colobus nrao edu inf...  \n2  columbia dsu edu columbia dsu edu columbia dsu...  \n3  galaxy gov morgan demon morgan demon hydro hyd...  \n4  antares humboldt kent edu humboldt kent edu an...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>class</th>\n      <th>preprocessed_text</th>\n      <th>preprocessed_subject</th>\n      <th>preprocessed_emails</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>From: rats@cbnewsc.cb.att.com (Morris the Cat)...</td>\n      <td>talk.politics.guns</td>\n      <td>I have sent this to Mr  privately Post it on...</td>\n      <td>VancouverSeattle Study Critiques</td>\n      <td>cbnewsc att</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>From: meunier@inf.enst.fr (Sylvain Meunier)\\nS...</td>\n      <td>comp.windows.x</td>\n      <td>I have sent this to Mr  privately Post it on...</td>\n      <td>Re XV 300 has escaped</td>\n      <td>inf enst colobus nrao edu colobus nrao edu inf...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>From: bitzm@columbia.dsu.edu (MICHAEL BITZ)\\nS...</td>\n      <td>misc.forsale</td>\n      <td>I have sent this to Mr  privately Post it on...</td>\n      <td>1 meg SIMMS for sale</td>\n      <td>columbia dsu edu columbia dsu edu columbia dsu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Subject: Re: Protective gear\\nFrom: bclarke@ga...</td>\n      <td>rec.motorcycles</td>\n      <td>I have sent this to Mr  privately Post it on...</td>\n      <td>Re Protective gear</td>\n      <td>galaxy gov morgan demon morgan demon hydro hyd...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From: jls@antares. (Jon Sweet)\\nSubject: Re: V...</td>\n      <td>comp.os.ms-windows.misc</td>\n      <td>I have sent this to Mr  privately Post it on...</td>\n      <td>Re VBRUN100dll</td>\n      <td>antares humboldt kent edu humboldt kent edu an...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\n\npreprocessed_data.to_csv('preprocessed_data (1)', encoding = 'utf-8-sig') \n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T11:03:04.396130Z","iopub.execute_input":"2022-04-03T11:03:04.396814Z","iopub.status.idle":"2022-04-03T11:03:10.771310Z","shell.execute_reply.started":"2022-04-03T11:03:04.396772Z","shell.execute_reply":"2022-04-03T11:03:10.770549Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/content/drive/MyDrive/preprocessed_data (1).csv')","metadata":{"id":"gndadepzVG9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"id":"F7ySOH3HVcuv","outputId":"b1829a29-2914-4652-ff4e-730bfa2428a4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=data[['text', 'preprocessed_text', 'preprocessed_subject', 'preprocessed_emails']]\ny=data['class']","metadata":{"id":"CVluMHluUF2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test= train_test_split(X,y,random_state=42,test_size=0.20,stratify=y)","metadata":{"id":"D3JnoLeiU99q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"id":"tQv8nk7gU-HT","outputId":"837192f0-9d01-4c35-b3dd-977935869aa7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model-1: Using 1D convolutions with word embeddings","metadata":{"id":"c0mwdtcvYv1X"}},{"cell_type":"markdown","source":"<pre>\n<b>Encoding of the Text </b> --> For a given text data create a Matrix with Embedding layer as shown Below. \nIn the example we have considered d = 5, but in this assignment we will get d = dimension of Word vectors we are using.\n i.e if we have maximum of 350 words in a sentence and embedding of 300 dim word vector, \n we result in 350*300 dimensional matrix for each sentance as output after embedding layer\n<img src='https://i.imgur.com/kiVQuk1.png'>\nRef: https://i.imgur.com/kiVQuk1.png\n\n<b>Reference:</b>\n<a href='https://stackoverflow.com/a/43399308/4084039'>https://stackoverflow.com/a/43399308/4084039</a>\n<a href='https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/'>https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/</a>\n\n<b><a href='https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work'>How EMBEDDING LAYER WORKS </a></b>\n\n</pre>\n\n### Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/","metadata":{"id":"gXPPsovJ3ePk"}},{"cell_type":"markdown","source":"<img src='https://i.imgur.com/fv1GvFJ.png'>\nref: 'https://i.imgur.com/fv1GvFJ.png'","metadata":{"id":"wGVQKge3Yv1e"}},{"cell_type":"markdown","source":"<pre>\n1. all are Conv1D layers with any number of filter and filter sizes, there is no restriction on this.\n\n2. use concatenate layer is to concatenate all the filters/channels. \n\n3. You can use any pool size and stride for maxpooling layer.\n\n4. Don't use more than 16 filters in one Conv layer becuase it will increase the no of params. \n( Only recommendation if you have less computing power )\n\n5. You can use any number of layers after the Flatten Layer.\n</pre>","metadata":{"id":"GC6SBG5AYv1f"}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle_tr = LabelEncoder()\nle_te = LabelEncoder()\n\ny_tr = le_tr.fit_transform(y_train)\ny_te = le_te.fit_transform(y_test)\nlab= y_tr\ny_train_label = tf.keras.utils.to_categorical(y_tr, 20)\ny_test_label = tf.keras.utils.to_categorical(y_te, 20)\n","metadata":{"id":"Fg09yOsaWnyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizing \ntok = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^`{|}~\\t\\n', oov_token=\"<OOV>\")\ntok.fit_on_texts(X_train['preprocessed_text'].astype(str))\n#Sequencing\ntxt_tr = X_train['preprocessed_text'].astype(str)\nseq_tr= tok.texts_to_sequences(txt_tr)\ntxt_te = X_test['preprocessed_text'].astype(str)\nseq_te= tok.texts_to_sequences(txt_te)","metadata":{"id":"msTesFbWWooS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#padding\npad_tr = pad_sequences(seq_tr, maxlen=1050, padding='post')\npad_te = pad_sequences(seq_te, maxlen=1050, padding='post')\n","metadata":{"id":"ZmcsOZVRWozT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#taken from reference notebook\nembeddings_index = {}\nf = open('/content/drive/MyDrive/glove.6B.100d.txt')\nfor line in f:\n    l= line.split()\n    val = l\n    coefs = np.asarray(val[1:], dtype='float32')\n    word = val[0]\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))\n","metadata":{"id":"mhuxxKCUW9-U","outputId":"f7b55f16-5dbd-4283-add3-641f2e8476f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#taken from reference notebook\nembedding_matrix = np.zeros((len(tok.word_index) + 1, 100))\nwrd_ind_items = tok.word_index.items()\nfor word, i in wrd_ind_items:\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n# words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","metadata":{"id":"rIyaNi5DW-DR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size=len(tok.word_index) + 1\nvocab_size","metadata":{"id":"WuuwpTrcXMR7","outputId":"6f0793de-16f3-4519-c236-bb0e9fdbef1c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Callbacks","metadata":{"id":"ZuL_9Z8hec6F"}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import f1_score\nclass F1score(tf.keras.callbacks.Callback):\n    def __init__(self,x_test,y_test):\n        super(F1score,self).__init__()\n        self.y_test=y_test\n        self.x_test=x_test\n        self.history={'auc': [],'F1_score':[]}\n        \n    def on_epoch_end(self, epoch, logs={}):\n        y_pred=self.model.predict(self.x_test)\n        mat=[]\n        f1_sc = 0\n        for val in y_pred:\n            y_val=np.argmax(val)\n            arr=np.zeros(20,dtype=int)\n            arr[y_val]=1\n            mat.append(arr)\n            y_pred_a = np.array(mat)\n        f1_sc=f1_score(self.y_test,y_pred_a,average='micro')\n        print(\"The F1 score for this epoch is: \",f1_sc)","metadata":{"id":"Vtj2VRAQXMWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Save model at every epoch if your validation accuracy is improved from previous epoch.\nfilepath = 'model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5'\nfrom tensorflow.keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(filepath=filepath, save_best_only = True, save_weights_only=False, monitor='val_accuracy', mode='max')","metadata":{"id":"Inf9HWJBdz13"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\nfrom tensorflow.keras.callbacks import TensorBoard\nimport datetime\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir = log_dir, histogram_freq=1)","metadata":{"id":"hjEoKx1fd42L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rate scheduler\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nlr_reduce = ReduceLROnPlateau(monitor = 'val_accuracy', factor=0.1, min_lr=0.001)\n\ndef scheduler(epoch, lr):\n  if( (epoch+1)%3 == 0):\n    lr = 0.5*lr\n    return lr\n  else:\n    return lr\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nsch_lr_reduce = LearningRateScheduler(scheduler, verbose=1)","metadata":{"id":"HJb9oCv_d5HP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ninput = Input(shape=(1050,),dtype='int32')\nemb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1050, trainable=True)(input)\n\nc1=Conv1D(3,11,kernel_initializer='glorot_uniform',activation='relu')(emb)\nc2=Conv1D(3,9,kernel_initializer='glorot_uniform',activation='relu')(emb)\nc3=Conv1D(3,7,kernel_initializer='glorot_uniform',activation='relu')(emb)\nconcat_1= concatenate([c1,c2,c3],axis=1)\n\nmp_1=MaxPooling1D(pool_size=2, padding='same')(concat_1)\n\ncc1=Conv1D(3,7,kernel_initializer='glorot_uniform',activation='relu')(mp_1)\ncc2=Conv1D(3,5,kernel_initializer='glorot_uniform',activation='relu')(mp_1)\ncc3=Conv1D(3,3,kernel_initializer='glorot_uniform',activation='relu')(mp_1)\nconcat_2 =concatenate([cc1, cc2, cc3],axis=1)\n\nmp_2=MaxPooling1D(pool_size=2, padding='same')(concat_2)\n\nconv_layer3 = Conv1D(3,12, activation='relu')(mp_2)\nflatten=Flatten()(conv_layer3)\ndp =Dropout(0.5)(flatten)\ndense_layer=Dense(100, activation='relu')(dp)\noutput_layer=Dense(20, activation='softmax')(dense_layer)\n\n","metadata":{"id":"C8_-WrPEXYVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam()\nf1score=F1score(x_test=pad_te, y_test=y_test_label)\ncallbacks_lst = [f1_score, checkpoint,tensorboard_callback]\n\nmodel = Model(inputs=input,outputs=output_layer)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())","metadata":{"id":"mXautKboiiWN","outputId":"cc8a34d5-31eb-4f1b-fde5-328a72d3cf3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks_lst = [f1score, tensorboard_callback]\nhistory=model.fit(pad_tr, y_train_label,validation_data=(pad_te, y_test_label),verbose=1,epochs=8, callbacks=callbacks_lst)","metadata":{"id":"K0mTgb4VmpOg","outputId":"10ea06d5-453c-450a-d919-afb87bff4ea4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs/fit","metadata":{"id":"RjEqcEDiH2vV","outputId":"954606ba-64d7-49dd-d656-be7988393763"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model,to_file='model.png',show_shapes=False,show_layer_names=True,rankdir='TB',expand_nested=False,dpi=96)","metadata":{"id":"GuUT_7eFXkrz","outputId":"aa54cdff-f6ec-454a-bd86-308326a74d3f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model-2 : Using 1D convolutions with character embedding","metadata":{"id":"9cg4L1V4Yv1d"}},{"cell_type":"markdown","source":"<pre>\n<pre><img src=\"https://i.ytimg.com/vi/CNY8VjJt-iQ/maxresdefault.jpg\" width=\"70%\">\nHere are the some papers based on Char-CNN\n 1. Xiang Zhang, Junbo Zhao, Yann LeCun. <a href=\"http://arxiv.org/abs/1509.01626\">Character-level Convolutional Networks for Text Classification</a>.NIPS 2015\n 2. Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. <a href=\"https://arxiv.org/abs/1508.06615\">Character-Aware Neural Language Models</a>. AAAI 2016\n 3. Shaojie Bai, J. Zico Kolter, Vladlen Koltun. <a href=\"https://arxiv.org/pdf/1803.01271.pdf\">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</a>\n 4. Use the pratrained char embeddings <a href='https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt'>https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt</a>\n</pre>","metadata":{"id":"2Djg4YVA3oQx"}},{"cell_type":"markdown","source":"<img src='https://i.imgur.com/EuuoJtr.png'>","metadata":{"id":"VXvKSEIeSvN5"}},{"cell_type":"code","source":"!rm -rf ./logs/","metadata":{"id":"QXAUh05CK4tJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_data = X_train['preprocessed_text'].astype(str)\nte_data = X_test['preprocessed_text'].astype(str)\ntok_2 = Tokenizer(char_level=True,num_words=None, oov_token='UNK')\n\ntok_2.fit_on_texts(tr_data)\nseq_char_tr= tok_2.texts_to_sequences(tr_data)\nseq_char_te= tok_2.texts_to_sequences(te_data)\n\ncd = {}\nalphabets = \"abcdefghijklmnopqrstuvwxyz\"\nfor j, chr in enumerate(alphabets):\n    cd[chr] = j + 1\ntok_2.word_index = cd.copy()\ntok_2.word_index[tok_2.oov_token] = max(cd.values()) + 1","metadata":{"id":"L-3h7_Evokdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length =1014\npad_tr_2 = pad_sequences(seq_char_tr, maxlen=max_length, padding='post')\npad_te_2 = pad_sequences(seq_char_te, maxlen=max_length, padding='post')","metadata":{"id":"lE_J-YhjZrM3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nvocab_size","metadata":{"id":"XopiCLB1Xc_Z","outputId":"c107230a-34d4-4032-f7f5-be1429c27ce5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_wt = []\nmat = np.zeros(vocab_size)\nvocab_size=len(tok_2.word_index)+1\nemb_wt.append(mat)\nprint(\"vocab size:\",vocab_size)\nfor char, i in tok_2.word_index.items():\n    onehot = np.zeros(vocab_size)\n    onehot[i - 1] = 1\n    emb_wt.append(onehot)\nemb_wt = np.array(emb_wt)\nembedding_weights = emb_wt","metadata":{"id":"iboYi89hXqK4","outputId":"172923dd-7bde-430b-af48-56859c4d31ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs=Input(shape=(1014,),dtype='int32')\nemb = Embedding(vocab_size,28, weights=[embedding_weights], input_length=1014, trainable=True)(inputs)\nc1=Conv1D(3,9,kernel_initializer='glorot_uniform',activation='relu')(emb)\nc2=Conv1D(3,7,kernel_initializer='glorot_uniform',activation='relu')(c1)\n\nmaxpool_1=MaxPooling1D(pool_size=2, strides=2, padding='same',)(c2)\n\ncc1=Conv1D(3,7,activation='relu',kernel_initializer='glorot_uniform')(maxpool_1)\ncc2=Conv1D(3,5,activation='relu',kernel_initializer='glorot_uniform')(cc1)\n\nmaxpool_2=MaxPooling1D(pool_size=2, strides=2, padding='same')(cc2)\n\nflatten=Flatten()(maxpool_2)\ndropout =Dropout(0.3)(flatten)\ndense_layer=Dense(32, activation='relu')(dropout)\noutput_layer=Dense(20, activation='softmax')(dense_layer)\n\n","metadata":{"id":"Wxs2sGiBpHeV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam()\nf1score_2=F1score(x_test=pad_te_2, y_test=y_test_label)\ncallbacks_lst = [f1score_2,tensorboard_callback]\n\nmodel_2 = Model(inputs=inputs,outputs=output_layer)\nmodel_2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model_2.summary())","metadata":{"id":"4KvmrentOHHh","outputId":"d3a37f08-3c13-45e8-d007-d13770f2d6c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history2=model_2.fit(pad_tr_2, y_train_label, epochs=8,validation_data=(pad_te_2, y_test_label),verbose=1,callbacks=callbacks_lst)\n","metadata":{"id":"aVPZWmNZbJKh","outputId":"1d1cd303-84e7-4bda-e7a2-91061e0dce4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs/fit","metadata":{"id":"7neiFt07XqUa","outputId":"b6d8743a-7d12-40f8-862c-47f502d949f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"V650XrV1okkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"qbDTu93NokpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"BO7WON66okuR"},"execution_count":null,"outputs":[]}]}